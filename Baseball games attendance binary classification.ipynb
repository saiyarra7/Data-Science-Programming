{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group work - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will focus on sports analytics. This data set is made available by http://www.baseball-reference.com. It contains data about professional baseball (MLB) games played in the 2016 season. There are 2,427 games in the data set. Each row represents a single game. The goal is to predict the attendance at a home teamâ€™s game. This is an important task because most franchises want to predict the number of attendees for a variety of reasons including profits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Variables\n",
    "\n",
    "The description of variables are provided in \"Baseball - Data Dictionary.docx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Use the **baseball.csv** data set and build a model to predict **attendance_binary**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission:\n",
    "\n",
    "Please save and submit this Jupyter notebook file. The correctness of the code matters for your grade. **Readability and organization of your code is also important.** You may lose points for submitting unreadable/undecipherable code. Therefore, use markdown cells to create sections, and use comments where necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended roles for group members:\n",
    "\n",
    "**Section 1:** to be completed by both group members\n",
    "\n",
    "**Section 2:** to be completed by the first group member and checked by the second\n",
    "\n",
    "**Section 3:** to be completed by the second group member and checked by the first\n",
    "\n",
    "**Important notes:**\n",
    "- Both group members will get the same grade. Therefore, you should check the work of your group member. If they make a mistake, you will be responsible for that mistake too.\n",
    "- Both group members must put in their fair share of effort. Otherwise, those who don't contribute to the assignment will not receive any grade.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: (6 points in total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep (5.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attendance_binary</th>\n",
       "      <th>previous_attendance</th>\n",
       "      <th>previous_away_team_errors</th>\n",
       "      <th>previous_away_team_hits</th>\n",
       "      <th>previous_away_team_runs</th>\n",
       "      <th>game_type</th>\n",
       "      <th>previous_game_type</th>\n",
       "      <th>previous_home_team_errors</th>\n",
       "      <th>previous_home_team_hits</th>\n",
       "      <th>previous_home_team_runs</th>\n",
       "      <th>game_day</th>\n",
       "      <th>previous_game_day</th>\n",
       "      <th>temperature</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>sky</th>\n",
       "      <th>previous_game_duration</th>\n",
       "      <th>previous_homewin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>43683</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>Night Game</td>\n",
       "      <td>Day Game</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Monday</td>\n",
       "      <td>55</td>\n",
       "      <td>24</td>\n",
       "      <td>Overcast</td>\n",
       "      <td>2.933333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>45785</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>Night Game</td>\n",
       "      <td>Day Game</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Monday</td>\n",
       "      <td>48</td>\n",
       "      <td>7</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>48282</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>Night Game</td>\n",
       "      <td>Day Game</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Monday</td>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>3.383333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>21830</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>Day Game</td>\n",
       "      <td>Night Game</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>In Dome</td>\n",
       "      <td>3.233333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>49289</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Night Game</td>\n",
       "      <td>Day Game</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Monday</td>\n",
       "      <td>81</td>\n",
       "      <td>12</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>2.633333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   attendance_binary  previous_attendance  previous_away_team_errors  \\\n",
       "0                  0                43683                          2   \n",
       "1                  0                45785                          0   \n",
       "2                  0                48282                          0   \n",
       "3                  0                21830                          0   \n",
       "4                  0                49289                          2   \n",
       "\n",
       "   previous_away_team_hits  previous_away_team_runs   game_type  \\\n",
       "0                        6                        2  Night Game   \n",
       "1                        7                        2  Night Game   \n",
       "2                        8                        4  Night Game   \n",
       "3                        9                        6    Day Game   \n",
       "4                        4                        2  Night Game   \n",
       "\n",
       "  previous_game_type  previous_home_team_errors  previous_home_team_hits  \\\n",
       "0           Day Game                          0                        6   \n",
       "1           Day Game                          0                       10   \n",
       "2           Day Game                          2                        4   \n",
       "3         Night Game                          0                       15   \n",
       "4           Day Game                          1                        1   \n",
       "\n",
       "   previous_home_team_runs   game_day previous_game_day  temperature  \\\n",
       "0                        6  Wednesday            Monday           55   \n",
       "1                        3  Wednesday            Monday           48   \n",
       "2                        3  Wednesday            Monday           65   \n",
       "3                       11  Wednesday           Tuesday           77   \n",
       "4                        3    Tuesday            Monday           81   \n",
       "\n",
       "   wind_speed       sky  previous_game_duration  previous_homewin  \n",
       "0          24  Overcast                2.933333                 1  \n",
       "1           7   Unknown                2.800000                 1  \n",
       "2          10    Cloudy                3.383333                 0  \n",
       "3           0   In Dome                3.233333                 1  \n",
       "4          12    Cloudy                2.633333                 1  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data\n",
    "# We will be predicting the attendance_binary variable\n",
    "baseball = pd.read_csv(r\"D:\\unknown\\Education\\Spring 22\\DSP\\Assignment 1\\baseball.csv\")\n",
    "baseball.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(baseball, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attendance_binary            0\n",
       "previous_attendance          0\n",
       "previous_away_team_errors    0\n",
       "previous_away_team_hits      0\n",
       "previous_away_team_runs      0\n",
       "game_type                    0\n",
       "previous_game_type           0\n",
       "previous_home_team_errors    0\n",
       "previous_home_team_hits      0\n",
       "previous_home_team_runs      0\n",
       "game_day                     0\n",
       "previous_game_day            0\n",
       "temperature                  0\n",
       "wind_speed                   0\n",
       "sky                          0\n",
       "previous_game_duration       0\n",
       "previous_homewin             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attendance_binary            0\n",
       "previous_attendance          0\n",
       "previous_away_team_errors    0\n",
       "previous_away_team_hits      0\n",
       "previous_away_team_runs      0\n",
       "game_type                    0\n",
       "previous_game_type           0\n",
       "previous_home_team_errors    0\n",
       "previous_home_team_hits      0\n",
       "previous_home_team_runs      0\n",
       "game_day                     0\n",
       "previous_game_day            0\n",
       "temperature                  0\n",
       "wind_speed                   0\n",
       "sky                          0\n",
       "previous_game_duration       0\n",
       "previous_homewin             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.isna().sum()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate the target variable and dropping them from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_set['attendance_binary']\n",
    "test_y = test_set['attendance_binary']\n",
    "\n",
    "train_inputs = train_set.drop(['attendance_binary'], axis=1)\n",
    "test_inputs = test_set.drop(['attendance_binary'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Identify the numerical and categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "previous_attendance            int64\n",
       "previous_away_team_errors      int64\n",
       "previous_away_team_hits        int64\n",
       "previous_away_team_runs        int64\n",
       "game_type                     object\n",
       "previous_game_type            object\n",
       "previous_home_team_errors      int64\n",
       "previous_home_team_hits        int64\n",
       "previous_home_team_runs        int64\n",
       "game_day                      object\n",
       "previous_game_day             object\n",
       "temperature                    int64\n",
       "wind_speed                     int64\n",
       "sky                           object\n",
       "previous_game_duration       float64\n",
       "previous_homewin               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the numerical columns\n",
    "numeric_columns = train_inputs.select_dtypes(include=[np.number]).columns.to_list()\n",
    "\n",
    "# Identify the categorical columns\n",
    "categorical_columns = train_inputs.select_dtypes('object').columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the data.head() we can see that one column previous_homewin is in binary so we'll have to convert it.\n",
    "binary_columns= ['previous_homewin']\n",
    "for col in binary_columns:\n",
    "    numeric_columns.remove(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns),\n",
    "        ('binary', binary_transformer, binary_columns)],\n",
    "        remainder='passthrough')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.12371621,  0.57666325, -0.20719118, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 1.05787587, -0.72716391, -0.78017264, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.38394295,  0.57666325, -1.35315411, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       ...,\n",
       "       [-0.52534761,  0.57666325, -0.78017264, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.94392488, -0.72716391,  0.65228102, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.98407336,  1.88049041, -0.49368191, ...,  1.        ,\n",
       "         0.        ,  1.        ]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x=preprocessor.fit_transform(train_inputs)\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0814842 ,  1.88049041, -1.06666338, ...,  1.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 1.17001331, -0.72716391, -1.63964484, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.29905768, -0.72716391, -0.20719118, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.49582715, -0.72716391,  1.22526249, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.26445059, -0.72716391, -0.20719118, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [-1.71775245, -0.72716391, -1.63964484, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the test data\n",
    "test_x = preprocessor.transform(test_inputs)\n",
    "\n",
    "test_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Baseline (0.5 point)\n",
    "# Baseline Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attendance_binary\n",
       "1                    873\n",
       "0                    825\n",
       "dtype: int64"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find majority class\n",
    "train_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attendance_binary\n",
       "1                    0.514134\n",
       "0                    0.485866\n",
       "dtype: float64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find percentage\n",
    "train_y.value_counts()/len(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: (3 points in total)\n",
    "\n",
    "Build three different SVM models (by changing the kernels, regularization, etc.). Generate their training and test values. Each model is worth 1 point. \n",
    "\n",
    "(Add cells as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=10, max_iter=100000)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_clf = LinearSVC(C=10, max_iter=100000) \n",
    "#We can also slove 'ConvergenceWarning: Liblinear failed to converge' warning by setting 'dual = False'\n",
    "\n",
    "svm_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8445229681978799"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Predict the train values\n",
    "train_y_pred = svm_clf.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8148148148148148"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = svm_clf.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[278,  65],\n",
       "       [ 70, 316]], dtype=int64)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#We usually create the confusion matrix on test set\n",
    "confusion_matrix(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.80       343\n",
      "           1       0.83      0.82      0.82       386\n",
      "\n",
      "    accuracy                           0.81       729\n",
      "   macro avg       0.81      0.81      0.81       729\n",
      "weighted avg       0.82      0.81      0.81       729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#We usually create the classification report on test set\n",
    "print(classification_report(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, coef0=1, degree=2, kernel='poly')"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "pol_svm2 = SVC(kernel=\"poly\", degree=2, coef0=1, C=1, gamma='scale')\n",
    "\n",
    "pol_svm2.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8733804475853946"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = pol_svm2.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8244170096021948"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = pol_svm2.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[283,  60],\n",
       "       [ 68, 318]], dtype=int64)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#We usually create the confusion matrix on test set\n",
    "confusion_matrix(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       343\n",
      "           1       0.84      0.82      0.83       386\n",
      "\n",
      "    accuracy                           0.82       729\n",
      "   macro avg       0.82      0.82      0.82       729\n",
      "weighted avg       0.82      0.82      0.82       729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#We usually create the classification report on test set\n",
    "print(classification_report(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model 3:SVC(kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_svm = SVC(kernel=\"rbf\", C=1, gamma='scale')\n",
    "\n",
    "rbf_svm.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8851590106007067"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = rbf_svm.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.821673525377229"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = rbf_svm.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[280,  63],\n",
       "       [ 67, 319]], dtype=int64)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#We usually create the confusion matrix on test set\n",
    "confusion_matrix(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.81       343\n",
      "           1       0.84      0.83      0.83       386\n",
      "\n",
      "    accuracy                           0.82       729\n",
      "   macro avg       0.82      0.82      0.82       729\n",
      "weighted avg       0.82      0.82      0.82       729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#We usually create the classification report on test set\n",
    "print(classification_report(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: (3 points in total)\n",
    "\n",
    "Build two different SGD models (by changing the penalty, etc. or adding polynomial terms) and one LogisticRregression model. Generate their training and test values. Each model is worth 1 point.\n",
    "\n",
    "(Add cells as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Model 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.1, max_iter=100, penalty=None, tol=0.0001)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier \n",
    "\n",
    "# tol = stopping criterion\n",
    "# eta0 = learning rate\n",
    "# penalty = regularization term\n",
    "# max_iter = number of passes over training data (i.e., epochs)\n",
    "\n",
    "sgd_logreg = SGDClassifier(max_iter=100, penalty=None, eta0=0.1, tol=0.0001) \n",
    "\n",
    "sgd_logreg.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.833922261484099"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = sgd_logreg.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7928669410150891"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = sgd_logreg.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[261,  82],\n",
       "       [ 69, 317]], dtype=int64)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.76      0.78       343\n",
      "           1       0.79      0.82      0.81       386\n",
      "\n",
      "    accuracy                           0.79       729\n",
      "   macro avg       0.79      0.79      0.79       729\n",
      "weighted avg       0.79      0.79      0.79       729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Model 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stochastic Gradient Descent version with penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.1, max_iter=100, tol=0.0001)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_logreg2 = SGDClassifier(max_iter=100, penalty='l2', eta0=0.1, tol=0.0001) \n",
    "\n",
    "sgd_logreg2.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8103651354534747"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = sgd_logreg2.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8106995884773662"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = sgd_logreg2.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[270,  73],\n",
       "       [ 65, 321]], dtype=int64)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80       343\n",
      "           1       0.81      0.83      0.82       386\n",
      "\n",
      "    accuracy                           0.81       729\n",
      "   macro avg       0.81      0.81      0.81       729\n",
      "weighted avg       0.81      0.81      0.81       729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "log_reg.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8433451118963486"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = log_reg.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.813443072702332"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = log_reg.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[279,  64],\n",
       "       [ 72, 314]], dtype=int64)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#We usually create the confusion matrix on test set\n",
    "confusion_matrix(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80       343\n",
      "           1       0.83      0.81      0.82       386\n",
      "\n",
      "    accuracy                           0.81       729\n",
      "   macro avg       0.81      0.81      0.81       729\n",
      "weighted avg       0.81      0.81      0.81       729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#We usually create the classification report on test set\n",
    "print(classification_report(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion (3 points in total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the train and test values of each model you built (1 point)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SVM model 1 (Linear):\n",
    "Trian Accuracy : 0.8445229681978799\n",
    "Test Accuracy :  0.8148148148148148\n",
    "\n",
    "SVM model 2 (Polynomial):\n",
    "Trian Accuracy : 0.8733804475853946\n",
    "Test Accuracy : 0.8244170096021948\n",
    "\n",
    "SVM model 3 (rbf):\n",
    "Trian Accuracy : 0.8851590106007067\n",
    "Test Accuracy : 0.821673525377229\n",
    "\n",
    "SGD model 1 (no penalty):\n",
    "Trian Accuracy : 0.833922261484099\n",
    "Test Accuracy : 0.7928669410150891\n",
    "\n",
    "SGD model 2 (l2 penalty):\n",
    "Trian Accuracy : 0.8103651354534747\n",
    "Test Accuracy : 0.8106995884773662\n",
    "\n",
    "Logistic regression:\n",
    "Trian Accuracy : 0.8433451118963486\n",
    "Test Accuracy : 0.813443072702332\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which model performs the best and why? (0.5 points) How does it compare to baseline? (0.5 points)\n",
    "\n",
    "Hint: The best model is the one that has the highest TEST score (regardless of any of the training values). If you select your model based on TRAIN values, you will lose points."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SVM Model 2:Polynomial Support vector machine model performs the best, giving us a test score accuracy of 0.82 because when compared to the other models it gave us the highest test accuracy scores. The model performed much better than the baseline value which is 0.514134."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there any evidence of overfitting in the best model, why or why not? If there is, what did you do about it? (0.5 points)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As we can see, there is no evidence of overfitting in the best model as the train and test accuracy scores are very close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there any evidence of overfitting in the other models (besides the best model), why or why not? If there is, what did you do about it? (0.5 points)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There is no evidence of overfittig in the other models as for all the models the train and test values are very close to each other. to say that there is evidence of overfitting there should be a considerable difference in the accuracy of train and test and the train accuracy will be too high when compare to the test values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
